

const express = require("express");
const { GoogleGenerativeAI } = require("@google/generative-ai");
const { HfInference } = require('@huggingface/inference');
const router = express.Router();

// Initialize Hugging Face
const hf = new HfInference(process.env.HUGGINGFACE_API_KEY);
const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY);
const model = genAI.getGenerativeModel({ model: "gemini-2.0-flash-exp" });
const AI_MODELS = {
  // Primary: Good for text generation and analysis
  primary: "meta-llama/Llama-2-7b-chat-hf",
  
  // Secondary: Good for classification
  secondary: "distilbert/distilbert-base-uncased-finetuned-sst-2-english",
  
  // Fallback: Always available
  fallback: "gpt2",
  
  // For zero-shot classification
  classifier: "facebook/bart-large-mnli"
};

// Original spam check endpoint
router.post("/check-report", async (req, res) => {
  try {
    const { 
      incidentDescription = "", 
      additionalNotes = "", 
      witnessAccount = "" 
    } = req.body;
    
    // Combine all text fields
    const combinedText = `
      Incident: ${incidentDescription}
      Notes: ${additionalNotes}
      Witness: ${witnessAccount}
    `.trim();
    
    console.log("üîç AI Checking text:", combinedText.substring(0, 100));
    
    // ===== STEP 1: Basic Validation =====
    const mainText = incidentDescription.trim();
    
    // Reject if completely empty
    if (!mainText || mainText.length < 3) {
      console.log("‚ùå Rejected: Too short");
      return res.json({ 
        allowed: false, 
        reason: "Please provide a meaningful description of the incident. Your report must contain at least a basic explanation of what happened." 
      });
    }
    
    // Reject obvious spam patterns
    const spamPatterns = [
      /^(asdf)+$/i,           // "asdf", "asdfasdf"
      /^(qwerty)+$/i,         // "qwerty"
      /^(test)+$/i,           // "test", "testtest"
      /^(hello)+$/i,          // "hello"
      /^(hi)+$/i,             // "hi"
      /^[0-9]+$/,             // Only numbers "12345"
      /^[a-z]{1,4}$/i,        // Single short word like "a", "the", "ok"
      /^(.)\1{4,}/,           // Repeated character "aaaaa"
      /^[!@#$%^&*()]+$/,      // Only symbols
      /^(lol|haha|hehe)+$/i,  // Just laughter
    ];
    
    const isSpamPattern = spamPatterns.some(pattern => pattern.test(mainText));
    
    if (isSpamPattern) {
      console.log("‚ùå Rejected: Spam pattern detected");
      return res.json({ 
        allowed: false, 
        reason: "Your report appears to contain random or meaningless text. Please provide a genuine description of the incident you're reporting." 
      });
    }
    
    // ===== STEP 2: AI Validation (Gemini) =====
    try {
      const prompt = `You are a content moderator for a university reporting system.

Analyze this incident report and determine if it's LEGITIMATE or SPAM/MEANINGLESS.

REJECT (return "spam") if:
- Random characters (asdf, qwerty, hjkl, etc.)
- Only numbers or symbols
- Nonsensical word salad
- Test messages ("test", "testing 123")
- Less than 10 words of meaningful content
- No clear description of an actual incident

APPROVE (return "legitimate") if:
- Describes a real incident (even if brief)
- Contains names, places, dates, or specific actions
- Has coherent sentences that tell a story
- Mentions people, events, or circumstances

Report text:
"${combinedText}"

Respond with ONLY ONE WORD: either "spam" or "legitimate"`;

      const result = await model.generateContent(prompt);
      const aiResponse = result.response.text().trim().toLowerCase();
      
      console.log("ü§ñ Gemini Response:", aiResponse);
      
      if (aiResponse.includes("spam")) {
        console.log("‚ùå Rejected by AI: Spam detected");
        return res.json({ 
          allowed: false, 
          reason: "Our AI system detected that your report lacks meaningful incident details. Please provide specific information about what happened, including who was involved, when it occurred, and what took place." 
        });
      }
      
      // If AI says "legitimate", approve
      console.log("‚úÖ Approved by AI");
      return res.json({ allowed: true });
      
    } catch (aiError) {
      console.error("‚ö†Ô∏è AI Error:", aiError.message);
      
      // If AI fails, use stricter fallback rules
      const wordCount = mainText.split(/\s+/).length;
      
      if (wordCount < 5) {
        console.log("‚ùå Rejected: Too few words (fallback)");
        return res.json({ 
          allowed: false, 
          reason: "Please provide more details about the incident. A valid report should include what happened, who was involved, and when it occurred." 
        });
      }
      
      // If AI fails but text seems okay, approve with caution
      console.log("‚úÖ Approved (AI unavailable, passed basic checks)");
      return res.json({ allowed: true });
    }
    
  } catch (error) {
    console.error("‚ùå Server Error:", error);
    
    // On server error, REJECT to be safe
    return res.json({ 
      allowed: false, 
      reason: "We're experiencing technical difficulties. Please try again in a moment." 
    });
  }
});


// ========== REAL AI ANALYSIS ENDPOINT ==========
router.post("/analyze-severity", async (req, res) => {
  try {
    const { 
      incidentDescription = "", 
      incidentTypes = [], 
      reportId = "unknown"
    } = req.body;
    
    console.log(`üîç REAL AI Analysis for report: ${reportId}`);
    console.log(`üìù Description: ${incidentDescription.substring(0, 100)}...`);
    
    // Validate input
    if (!incidentDescription || incidentDescription.trim().length < 10) {
      return res.status(400).json({
        success: false,
        message: "Valid incident description (minimum 10 characters) is required"
      });
    }
    
    const cleanDescription = incidentDescription.trim().substring(0, 1000);
    
    // ========== REAL AI LOGIC ==========
    let analysis;
    
    // Method 1: Try zero-shot classification (most accurate)
    try {
      console.log("üéØ Using zero-shot classification...");
      analysis = await analyzeWithZeroShot(cleanDescription);
    } catch (zeroError) {
      console.log("‚ö†Ô∏è Zero-shot failed, trying content analysis...");
      analysis = analyzeContentIntelligently(cleanDescription, incidentTypes);
    }
    
    // Add metadata
    analysis.analyzedAt = new Date().toISOString();
    analysis.analysisId = `ai_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
    analysis.provider = "huggingface";
    
    console.log(`‚úÖ REAL AI Analysis Complete: ${analysis.severity} (${analysis.confidence * 100}% confidence)`);
    
    return res.json({
      success: true,
      message: "AI severity analysis completed",
      data: analysis,
      reportId: reportId,
      timestamp: new Date().toISOString()
    });
    
  } catch (error) {
    console.error("‚ùå AI Error:", error.message);
    
    const fallback = createFallbackAnalysis(req.body.incidentDescription || '');
    
    return res.json({
      success: true,
      message: "Analysis completed with fallback",
      data: fallback,
      isFallback: true,
      reportId: req.body.reportId || "unknown",
      timestamp: new Date().toISOString()
    });
  }
});

// ========== ZERO-SHOT CLASSIFICATION (MOST ACCURATE) ==========
async function analyzeWithZeroShot(description) {
  console.log("ü§ñ Calling Hugging Face API...");
  
  const result = await hf.zeroShotClassification({
    model: "facebook/bart-large-mnli",
    inputs: [description.substring(0, 500)],
    parameters: {
      candidate_labels: [
        "SEVERE: violence, weapons, threats, assault, emergency, danger, injury, attack, fire, suicide",
        "MODERATE: harassment, bullying, conflict, argument, stress, anxiety, concern, theft, damage",
        "MILD: complaint, inquiry, administrative, minor issue, noise, question, routine"
      ],
      multi_label: false
    }
  });
  
  const classification = result[0];
  const topLabel = classification.labels[0];
  const confidence = classification.scores[0];
  
  console.log(`üìä Classification Results:`);
  console.log(`  1. ${classification.labels[0]}: ${(classification.scores[0]*100).toFixed(1)}%`);
  console.log(`  2. ${classification.labels[1]}: ${(classification.scores[1]*100).toFixed(1)}%`);
  console.log(`  3. ${classification.labels[2]}: ${(classification.scores[2]*100).toFixed(1)}%`);
  
  // Determine severity from classification
  let severity;
  if (topLabel.includes("SEVERE")) {
    severity = "SEVERE";
  } else if (topLabel.includes("MODERATE")) {
    severity = "MODERATE";
  } else {
    severity = "MILD";
  }
  
  // Generate analysis based on severity
  return createAnalysisFromSeverity(severity, confidence, description);
}

// ========== INTELLIGENT CONTENT ANALYSIS ==========
function analyzeContentIntelligently(description, incidentTypes) {
  console.log("üß† Analyzing content intelligently...");
  
  const text = description.toLowerCase();
  
  // Score different aspects
  const scores = {
    severe: 0,
    moderate: 0,
    mild: 0
  };
  
  // Check for SEVERE indicators
  if (/(emergency|urgent|911|police|ambulance|fire)/i.test(text)) scores.severe += 3;
  if (/(violence|assault|attack|hit|punch|kick|stab|shoot)/i.test(text)) scores.severe += 2.5;
  if (/(weapon|gun|knife|bat|firearm)/i.test(text)) scores.severe += 2;
  if (/(threat|kill|murder|hurt|harm|danger)/i.test(text)) scores.severe += 1.5;
  if (/(sexual|rape|molest|abuse)/i.test(text)) scores.severe += 2;
  if (/(suicide|self.?harm|die|death)/i.test(text)) scores.severe += 2;
  
  // Check for MODERATE indicators
  if (/(bully|harass|tease|mock|insult)/i.test(text)) scores.moderate += 1.5;
  if (/(discriminat|racist|prejudice|bias)/i.test(text)) scores.moderate += 1.5;
  if (/(stalk|follow|watch|obsess)/i.test(text)) scores.moderate += 1.3;
  if (/(anxiety|depress|stress|panic|trauma)/i.test(text)) scores.moderate += 1;
  if (/(steal|theft|rob|burglar)/i.test(text)) scores.moderate += 1;
  if (/(damage|break|vandal|destroy)/i.test(text)) scores.moderate += 0.8;
  if (/(cheat|plagiar|fraud|dishonest)/i.test(text)) scores.moderate += 0.8;
  
  // Check for MILD indicators
  if (/(complain|noise|loud|music|party)/i.test(text)) scores.mild += 0.5;
  if (/(question|inquire|ask|information)/i.test(text)) scores.mild += 0.3;
  if (/(late|absent|attendance)/i.test(text)) scores.mild += 0.2;
  if (/(clean|dirty|messy|maintenance)/i.test(text)) scores.mild += 0.2;
  if (/(lost|found|missing|item)/i.test(text)) scores.mild += 0.2;
  
  console.log(`üìä Content Analysis Scores:`, scores);
  
  // Determine final severity
  let severity, confidence;
  
  if (scores.severe >= 2) {
    severity = "SEVERE";
    confidence = 0.8 + (scores.severe * 0.05);
  } else if (scores.moderate >= 1.5) {
    severity = "MODERATE";
    confidence = 0.7 + (scores.moderate * 0.04);
  } else if (scores.mild >= 1) {
    severity = "MILD";
    confidence = 0.65 + (scores.mild * 0.03);
  } else {
    // Default to moderate if unclear
    severity = "MODERATE";
    confidence = 0.7;
  }
  
  // Cap confidence
  confidence = Math.min(confidence, 0.95);
  
  return createAnalysisFromSeverity(severity, confidence, description);
}

// ========== CREATE ANALYSIS FROM SEVERITY ==========
function createAnalysisFromSeverity(severity, confidence, description) {
  const text = description.toLowerCase();
  
  // Generate explanation based on severity
  let explanation;
  switch (severity) {
    case 'SEVERE':
      if (/(violence|assault|attack)/i.test(text)) {
        explanation = "Report describes violent behavior requiring immediate safety intervention.";
      } else if (/(weapon|gun|knife)/i.test(text)) {
        explanation = "Weapon involvement indicates serious safety threat needing urgent response.";
      } else if (/(threat|danger|kill)/i.test(text)) {
        explanation = "Threats to safety identified requiring immediate attention.";
      } else if (/(emergency|urgent|911)/i.test(text)) {
        explanation = "Emergency situation reported requiring immediate response.";
      } else if (/(sexual|rape|molest)/i.test(text)) {
        explanation = "Sexual misconduct reported requiring sensitive and urgent handling.";
      } else if (/(suicide|self.?harm)/i.test(text)) {
        explanation = "Mental health emergency requiring immediate intervention.";
      } else {
        explanation = "Serious safety concerns identified requiring prompt investigation.";
      }
      break;
      
    case 'MODERATE':
      if (/(bully|harass|tease)/i.test(text)) {
        explanation = "Harassment or bullying behavior reported requiring investigation.";
      } else if (/(discriminat|racist|prejudice)/i.test(text)) {
        explanation = "Discrimination concerns identified needing follow-up.";
      } else if (/(steal|theft|damage)/i.test(text)) {
        explanation = "Property issues reported requiring administrative action.";
      } else if (/(conflict|argue|fight)/i.test(text)) {
        explanation = "Interpersonal conflicts identified needing mediation.";
      } else if (/(anxiety|depress|stress)/i.test(text)) {
        explanation = "Mental health concerns reported requiring support services.";
      } else {
        explanation = "Concerning behavior reported that warrants investigation.";
      }
      break;
      
    case 'MILD':
      if (/(complain|noise)/i.test(text)) {
        explanation = "Routine complaint reported that can be addressed through standard procedures.";
      } else if (/(question|inquire|ask)/i.test(text)) {
        explanation = "General inquiry or information request.";
      } else if (/(administrative|procedure|process)/i.test(text)) {
        explanation = "Administrative matter requiring standard handling.";
      } else {
        explanation = "Minor concern reported that can be resolved routinely.";
      }
      break;
  }
  
  // Extract keywords
  const keywords = extractKeywords(description);
  
  // Generate risk factors
  const riskFactors = [];
  if (severity === 'SEVERE') {
    riskFactors.push("Safety threat identified");
    if (/(weapon|violence|threat)/i.test(text)) riskFactors.push("Potential for harm");
    riskFactors.push("Requires immediate intervention");
  } else if (severity === 'MODERATE') {
    riskFactors.push("Behavioral concern detected");
    riskFactors.push("Potential for escalation");
    riskFactors.push("Needs investigation");
  } else {
    riskFactors.push("Routine matter");
    riskFactors.push("Standard procedure applicable");
    riskFactors.push("Low risk");
  }
  
  // Generate suggested actions
  const suggestedActions = [];
  if (severity === 'SEVERE') {
    suggestedActions.push("Immediate safety assessment");
    suggestedActions.push("Contact campus security if needed");
    suggestedActions.push("Priority investigation within 24 hours");
  } else if (severity === 'MODERATE') {
    suggestedActions.push("Schedule investigation within 3-7 days");
    suggestedActions.push("Interview involved parties");
    suggestedActions.push("Document findings thoroughly");
  } else {
    suggestedActions.push("Follow standard procedures");
    suggestedActions.push("Maintain records");
    suggestedActions.push("Provide appropriate follow-up");
  }
  
  // Calculate priority score
  let priorityScore;
  switch (severity) {
    case 'SEVERE':
      priorityScore = 90 + Math.round(confidence * 10);
      break;
    case 'MODERATE':
      priorityScore = 70 + Math.round(confidence * 20);
      break;
    case 'MILD':
      priorityScore = 50 + Math.round(confidence * 20);
      break;
  }
  
  return {
    severity: severity,
    confidence: Math.round(confidence * 100) / 100,
    explanation: explanation,
    keywords: keywords,
    riskFactors: riskFactors,
    recommendedResponseTime: severity === 'SEVERE' ? '24 hours' : 
                           severity === 'MODERATE' ? '3-7 days' : '7-14 days',
    priorityScore: Math.min(priorityScore, 100),
    suggestedActions: suggestedActions,
    modelUsed: "facebook/bart-large-mnli",
    method: "zero-shot-classification"
  };
}

// ========== HELPER FUNCTIONS ==========
function extractKeywords(description) {
  const words = description.toLowerCase().split(/\s+/);
  const commonWords = new Set(['the', 'and', 'for', 'that', 'this', 'with', 'was', 'were', 'from']);
  
  const keywords = words
    .filter(word => word.length > 3 && !commonWords.has(word))
    .map(word => word.replace(/[^a-z]/g, ''))
    .filter(word => word.length > 3)
    .slice(0, 5);
  
  if (keywords.length < 3) {
    keywords.push('incident', 'report', 'university');
  }
  
  return [...new Set(keywords)].slice(0, 5);
}

function createFallbackAnalysis(description) {
  return {
    severity: "MODERATE",
    confidence: 0.7,
    explanation: "Analysis completed with fallback method.",
    keywords: ["fallback", "analysis", "report"],
    riskFactors: ["System fallback", "Manual review recommended"],
    recommendedResponseTime: "3-7 days",
    priorityScore: 70,
    suggestedActions: ["Review manually", "Follow standard procedure"],
    analyzedAt: new Date().toISOString(),
    analysisId: `fallback_${Date.now()}`,
    modelUsed: "fallback",
    isFallback: true
  };
}

module.exports = router;